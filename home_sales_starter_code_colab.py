# -*- coding: utf-8 -*-
"""Home_Sales_starter_code_colab.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Sfy_mFIM5z1Cn2YlQ3-R5cT0YMWC83hE
"""

import os
# Find the latest version of spark 3.x  from http://www.apache.org/dist/spark/ and enter as the spark version
# For example:
# spark_version = 'spark-3.4.0'
spark_version = 'spark-3.5.0'
os.environ['SPARK_VERSION']=spark_version

# Install Spark and Java
!apt-get update
!apt-get install openjdk-11-jdk-headless -qq > /dev/null
!wget -q http://www.apache.org/dist/spark/$SPARK_VERSION/$SPARK_VERSION-bin-hadoop3.tgz
!tar xf $SPARK_VERSION-bin-hadoop3.tgz
!pip install -q findspark

# Set Environment Variables
os.environ["JAVA_HOME"] = "/usr/lib/jvm/java-11-openjdk-amd64"
os.environ["SPARK_HOME"] = f"/content/{spark_version}-bin-hadoop3"

# Start a SparkSession
import findspark
findspark.init()

# Import packages
from pyspark.sql import SparkSession
import time
from pyspark.sql import Row
from pyspark.sql.types import StructType,StructField,StructField,StructType,DateType,IntegerType

# Create a SparkSession
spark = SparkSession.builder.appName("SparkSQL").getOrCreate()

# 1. Read in the AWS S3 bucket into a DataFrame.
from pyspark import SparkFiles
url = "https://2u-data-curriculum-team.s3.amazonaws.com/dataviz-classroom/v1.2/22-big-data/home_sales_revised.csv"

spark.sparkContext.addFile(url)

df = spark.read.option('header' , 'true').csv(SparkFiles.get("home_sales_revised.csv"), inferSchema=True, sep= ',' ,timestampFormat="yyyy-MM-dd-HH.mm" )

# 2. Create a temporary view of the DataFrame.

df.createOrReplaceGlobalTempView('home_sales')
df.show()

from ast import alias
# 3. What is the average price for a four bedroom house sold in each year rounded to two decimal places?
from pyspark.sql.functions import mean
from pyspark.sql.functions import round , col
four_bed_per_year = df.filter(df.bedrooms ==4)\
    .groupBy("date")\
    .agg(round(mean("price"), 2).alias("avg_price"))

four_bed_per_year.show()

# 4. What is the average price of a home for each year the home was built that have 3 bedrooms and 3 bathrooms rounded to two decimal places?
three_bed_three_bath_per_year = df.filter((df.bedrooms == 3) & (df.bathrooms == 3)) \
 .groupBy("date") \
  .agg(round(mean("price"), 2).alias("avg_price"))


three_bed_three_bath_per_year.show()

# 5. What is the average price of a home for each year built that have 3 bedrooms, 3 bathrooms, with two floors,
# and are greater than or equal to 2,000 square feet rounded to two decimal places?
avg_price_per_year = df.filter(
    (df.bedrooms == 3) &
    (df.bedrooms == 3) &
    (df.floors == 2) &
    (df.sqft_living >= 2000)
) \
     .groupBy("date") \
     .agg(round(mean("price"), 2).alias("avg_price"))
avg_price_per_year.show()

# 6. What is the "view" rating for the average price of a home, rounded to two decimal places, where the homes are greater than
# or equal to $350,000? Although this is a small dataset, determine the run time for this query.

start_time = time.time()



print("--- %s seconds ---" % (time.time() - start_time))

# 7. Cache the the temporary table home_sales.
spark.catalog.cacheTable("global_temp.home_sales")

# 8. Check if the table is cached.
spark.catalog.isCached("global_temp.home_sales")

# 9. Using the cached data, run the query that filters out the view ratings with average price
#  greater than or equal to $350,000. Determine the runtime and compare it to uncached runtime.

start_time = time.time()



print("--- %s seconds ---" % (time.time() - start_time))

# 10. Partition by the "date_built" field on the formatted parquet home sales data
df.write.partitionBy('date_built').parquet('p_sales' , mode ='overwrite')

# 11. Read the parquet formatted data.
p_df = spark.read.parquet('p_sales')

# 12. Create a temporary table for the parquet data.
p_df.createOrReplaceGlobalTempView('p_sales')

# 13. Run the query that filters out the view ratings with average price of greater than or equal to $350,000
# with the parquet DataFrame. Round your average to two decimal places.
# Determine the runtime and compare it to the cached version.

start_time = time.time()



print("--- %s seconds ---" % (time.time() - start_time))

# 14. Uncache the home_sales temporary table.
spark.catalog.uncacheTable("global_temp.home_sales")

# 15. Check if the home_sales is no longer cached
spark.stop()

